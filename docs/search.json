[
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Services",
    "section": "",
    "text": "Below are the primary service buckets and typical engagement models I offer. If you don’t see exactly what you need, I usually tailor a small pilot to prove value quickly.\n\n\n\n\n\n\nExecutive dashboards, KPI design, forecasting, automation, and decision plumbing for executive teams.\n\n\n\nSite selection, territory design, drive-time & catchment analysis, and network optimization.\n\n\n\nChange detection, asset monitoring, land-cover classification, NDVI & index time-series, and coastal risk analysis.\n\n\n\nDemand scoring, risk heatmaps, suitability analysis, scenario planning, and automated monitoring pipelines.\n\n\n\n\n\n\n\n\n\nAudit of data, quick wins, and a short roadmap with an actionable prototype.\n\n\n\nScoped outcome: a dashboard, report, or monitoring workflow delivered end-to-end.\n\n\n\nOngoing updates, alerts, and feature improvements to keep the solution current.\n\n\n\n\nRequest a proposal\n\n\n\n\n\n\nI work across sectors where location, risk, and change matter. Below are common examples and short mini-cases.\n\n\n\nMap corridor routing and encroachment risk around critical assets. Prioritise maintenance with satellite-detected change.\nMini case: Pipeline Corridor Risk Mapping — combined easement data, SAR change alerts, and field tickets to flag hotspots.\n\n\n\nScore sites with demand, drive-time, and competition. Design territories to reduce cannibalisation and improve coverage.\nMini case: Multi-Site Expansion Playbook — tract scoring and trade-area maps delivered as an executive shortlist.\n\n\n\nOptimize territories and routes to lower miles and stabilise ETAs. Visualise service coverage for capacity planning.\nMini case: Distributor Territory Redesign — rebalanced zones and removed route overlap with clear KPIs.\n\n\n\nMonitor shoreline change and wetlands with time-series remote sensing and produce compliance-ready reporting.\nMini case: Coastal Erosion Monitoring Pilot — NDWI/NDVI trends with an alerting dashboard.\n\n\n\nMap crop health and variability (NDVI anomalies) and delineate yield zones for targeted interventions.\nMini case: Field Variability Mapping — Sentinel-2 indices guiding in-field checks and inputs.\n\n\n\nPortfolio exposure mapping with hazard overlays and event monitoring for rapid triage.\nMini case: Portfolio Exposure Heatmaps — unified parcel-level risk layers for underwriting reviews.\n\n\n\n\nRequest a proposal"
  },
  {
    "objectID": "services.html#industries-served",
    "href": "services.html#industries-served",
    "title": "Services",
    "section": "",
    "text": "I work across sectors where location, risk, and change matter. Below are common examples and short mini-cases.\n\n\n\nMap corridor routing and encroachment risk around critical assets. Prioritise maintenance with satellite-detected change.\nMini case: Pipeline Corridor Risk Mapping — combined easement data, SAR change alerts, and field tickets to flag hotspots.\n\n\n\nScore sites with demand, drive-time, and competition. Design territories to reduce cannibalisation and improve coverage.\nMini case: Multi-Site Expansion Playbook — tract scoring and trade-area maps delivered as an executive shortlist.\n\n\n\nOptimize territories and routes to lower miles and stabilise ETAs. Visualise service coverage for capacity planning.\nMini case: Distributor Territory Redesign — rebalanced zones and removed route overlap with clear KPIs.\n\n\n\nMonitor shoreline change and wetlands with time-series remote sensing and produce compliance-ready reporting.\nMini case: Coastal Erosion Monitoring Pilot — NDWI/NDVI trends with an alerting dashboard.\n\n\n\nMap crop health and variability (NDVI anomalies) and delineate yield zones for targeted interventions.\nMini case: Field Variability Mapping — Sentinel-2 indices guiding in-field checks and inputs.\n\n\n\nPortfolio exposure mapping with hazard overlays and event monitoring for rapid triage.\nMini case: Portfolio Exposure Heatmaps — unified parcel-level risk layers for underwriting reviews.\n\n\n\n\nRequest a proposal"
  },
  {
    "objectID": "project_writeups/waffle-house-index.html",
    "href": "project_writeups/waffle-house-index.html",
    "title": "Simple R Application with Spatial Data",
    "section": "",
    "text": "Project description: Created as a proof of concept, this simplistic R Shiny app gives a good example of what is possible for presenting and sharing geospatial data outside of the ArcWeb framework.\n\n\n\nI like to use this app as an example as it is extremely quick to code up but combines a few interesting features such as:\n\nReal time CRS coordinate transforms and calculations.\nWeb scraping of a current data set from a non-research oriented page.\nReactive user input.\nSimple sharing of GIS based applications\n\nThis app is designed to simulate the now defunct Waffle House Index once hosted on the FEMA website. Waffle House is a restaurant in the southern USA. It rarely closes even in the face of extreme circumstances such as hurricanes as they are capable of running a reduced menu with no electricity. As such, FEMA used to monitor the operating conditions of Waffle Houses in order to judge the severity of local disaster conditions. Waffle house has three states of operation, Green for as normal, Yellow for reduced capacity menu, Red for full closure. My application scrapes the current locations for all Waffle Houses in operation in the USA via the Waffle House website, displays their location on a simple map, and allows the user to click on the map to create a false disaster area with a bespoke killzone and wounded radius. Displaying these radii requires on the spot transformation between the WGS84 and ESPG 9822 projections, geographic distance calculation, and image display. This application can be shared simply via its URL and requires no setup from end users.\nThough this application is a bit tongue in cheek and extremely barebones, it shows that sharable, interactive, GIS based applications can be made quickly to present ideas and data in ways that capture your audience’s attention. Data science is an extremely exciting field and the conclusions we draw can be world changing but if we cannot transmit the knowledge we discover in ways palatable to non-analysts, then no matter how groundbreaking our research, no one will pay it any mind. This R shiny app template proves that, with the absolute minimum of time investment, we can create visualizations that better promote end consumer engagement and understanding of our results.\nTry out the app here. Waffle House Index Simulator."
  },
  {
    "objectID": "project_writeups/waffle-house-index.html#simple-r-application-with-spatial-data",
    "href": "project_writeups/waffle-house-index.html#simple-r-application-with-spatial-data",
    "title": "Simple R Application with Spatial Data",
    "section": "",
    "text": "Project description: Created as a proof of concept, this simplistic R Shiny app gives a good example of what is possible for presenting and sharing geospatial data outside of the ArcWeb framework.\n\n\n\nI like to use this app as an example as it is extremely quick to code up but combines a few interesting features such as:\n\nReal time CRS coordinate transforms and calculations.\nWeb scraping of a current data set from a non-research oriented page.\nReactive user input.\nSimple sharing of GIS based applications\n\nThis app is designed to simulate the now defunct Waffle House Index once hosted on the FEMA website. Waffle House is a restaurant in the southern USA. It rarely closes even in the face of extreme circumstances such as hurricanes as they are capable of running a reduced menu with no electricity. As such, FEMA used to monitor the operating conditions of Waffle Houses in order to judge the severity of local disaster conditions. Waffle house has three states of operation, Green for as normal, Yellow for reduced capacity menu, Red for full closure. My application scrapes the current locations for all Waffle Houses in operation in the USA via the Waffle House website, displays their location on a simple map, and allows the user to click on the map to create a false disaster area with a bespoke killzone and wounded radius. Displaying these radii requires on the spot transformation between the WGS84 and ESPG 9822 projections, geographic distance calculation, and image display. This application can be shared simply via its URL and requires no setup from end users.\nThough this application is a bit tongue in cheek and extremely barebones, it shows that sharable, interactive, GIS based applications can be made quickly to present ideas and data in ways that capture your audience’s attention. Data science is an extremely exciting field and the conclusions we draw can be world changing but if we cannot transmit the knowledge we discover in ways palatable to non-analysts, then no matter how groundbreaking our research, no one will pay it any mind. This R shiny app template proves that, with the absolute minimum of time investment, we can create visualizations that better promote end consumer engagement and understanding of our results.\nTry out the app here. Waffle House Index Simulator."
  },
  {
    "objectID": "project_writeups/presentations.html",
    "href": "project_writeups/presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Tech: Presentation\n\nArchaeological Predictive Mapping\nA long presentation about the development process of my thesis on the use of machine learning for the construction of archaeological predictive maps.\nView slides\n\n\n\n\nTech: Presentation\n\nAutomated Geomorphometric Sexual Dimorphism Classification\nAn overview of my methodology for improving the workflow at the University of Tübingen for the classification of sexual dimorphism based on the human viscerocranium.\nView slides\n\n\n\n\nTech: Presentation\n\nSatellite Based Archaeology - I & II\nA two-part presentation on the use of satellite-based data sources for terrestrial and maritime mapping in archaeology.\nView slides\n\n\n\n\nTech: Presentation\n\nHuman Osteological Taphonomy - Industrial War\nAn overview of human osteological taphonomy as it relates to aspects of ancient and modern warfare.\nView slides"
  },
  {
    "objectID": "project_writeups/pca-missing-data.html",
    "href": "project_writeups/pca-missing-data.html",
    "title": "PCA to Identify Missing Data Values",
    "section": "",
    "text": "Project description: I was interested in looking at some osteological data. Howells craniometric data set is freely available and relatively large: 2524 human individuals over 28 populations. There is supposed to be a description of the structure of the data available on the source site but when I accessed it that link was broken. Knowing little else than that the data had 82 separate craniological measurements, I ran a PCA to see if any clustering or trends evolved in PC space so I could analyze a reduced number of dimensions. This resulted in the biplot below and motivated me to run down the cause of such extreme clustering behavior.\n\n\n\nThe data from the source site is downloadable as two groups. One is the normal Howells crania (2524 crania) and the other is the so-called test set (524 crania). The latter is a collection of crania that were either not whole or had some other deficiency that differentiated them from the normative homo sapien skull. I worked with the normal data set as it had more individuals and as a novice to the world of craniometry I wanted to keep my data as representative of the norm as possible. The Howells set has 82 craniometric measurements in mm, variables for Population in both numeric and string form, Sex, and an individual ID.\n\n\n\n\nAfter generating the first biplot I created the following figure to see if this behavior continued with other PC components:\n\nAt this point it was obvious that PC2 was driving the clustering so I made the variable vector plot below to see if anything jumped out:\n\nThe cluster of highly positive vectors seemed to be the cause of the clustering but there were too many variables in the plot to figure out their names visually. From here I looked into the loadings on PC2 and discovered that the 11 largest were an order of magnitude larger than all the rest. As I was new to craniometrics I didn’t know the conversion from the three letter codes to what the measurements actually meant. I found this publication that described the codes and even mentioned that the variables I was interested in apparently came from a later study. I converted the pdf to .csv tables, read them into R, and added them to the Howells set:\n\nFrom here, I separated the original data set by their position along the PC2 axis in order to grab the two clusters. I then ran a Welch’s t-test to check for significant differences in the means of the different variables. This resulted in some strange results where the t-tests reported means of zero for some variable columns. Looking at the smaller groups data frame I was able to see that there were missing values represented as 0 in those variable columns. The missing data columns were the same variables that had the high loading values in PC space so everything started making sense.\n\nThe missing data was causing a ton of variability. However, there were only 662 individuals with missing values and 1862 individuals with complete data. I believe this is why the variation effect was captured in PC2 as the missing data individuals comprised only roughly 1/5 of the total data.\n\n\n\nSo how to approach this analytically? I could rip apart the R PCA algorithm and take the equations into pure number space but that would remove the visual component that first drew me to this behavior. Instead I decided to plot the PCA results after manipulating a test set of data. I made a data frame of the first 1000 individuals in the Howells data set with only the first 10 craniological measurements. Next I iterated over the frame taking one craniological measurement column and causing 100 then 500 then 900 of the data points in that column to be zero. Then I ran PCAs over these subsets and plotted them. After that, I added another column in, repeated the missing data adjustments, and ran the PCAs. I did this until I saw the clustering move fully into the first PC. The plots are below:\nOne Variable Manipulated:\n  \nTwo Variables:\n  \nThree Variables:\n  \nFour Variables:\n  \nFinally we start making sense of the data. PC components are affected by outlier groups. For each group of plots, the middle plot corresponding to 500, half, of the manipulated columns’ data points are set to 0. But since half the data is affected, the PCA still shows relatively normal clustering behavior as the missing data isn’t a small outlier group, it is now considered a large and thus valid trend in the data space resulting in normative clustering. The 100 and 900 missing data point groups (plots 1 and 3 in each set of 3) create outliers of either missing data or complete data.\nSeeing how the first set of plots (only one manipulated variable column out of 10 total) shows the clustering effect in PC3, the second and third sets (2 and 3 manipulated variables respectively) show clustering in PC2, and the fourth set (4 manipulated columns) show clustering in PC1. This means that the PC that clustering due to missing data set to some arbitrary value occurs in is determined by the number of variables affected by the missing data. For this data set we saw that fewer affected variables meant that this clustering moved into higher PC components.\n\n\n\nOk so what? Why should we care about missing data like this when there are tons of ways to find missing values. To me this opens the door for some fun experiments in PC space. I’ve described the clustering behavior visually here but if we attacked this purely mathematically it might be interesting to see the exact factors that affect which PC shows clustering and the divisions between PC clustering levels. I have a feeling it is a function of n number of variables with k missing values but there could be some fascinating behaviors with specially constructed test sets. I may see what happens with this if, instead of missing data, I “corrupt” some variables with increasing random noise. Regardless, this little foray into craniometric PC analysis has shown that when confronted with interesting behavior, even if that behavior is driven by badly collected data, it is worth investigating its source.\nHere at the end I will freely admit there are easier ways to come to these conclusions, heck there are even easier numerical ways to look for missing data. However, to me at least, there is something beautiful about a visual solution to a numbers based problem."
  },
  {
    "objectID": "project_writeups/pca-missing-data.html#pca-to-identify-missing-data-values",
    "href": "project_writeups/pca-missing-data.html#pca-to-identify-missing-data-values",
    "title": "PCA to Identify Missing Data Values",
    "section": "",
    "text": "Project description: I was interested in looking at some osteological data. Howells craniometric data set is freely available and relatively large: 2524 human individuals over 28 populations. There is supposed to be a description of the structure of the data available on the source site but when I accessed it that link was broken. Knowing little else than that the data had 82 separate craniological measurements, I ran a PCA to see if any clustering or trends evolved in PC space so I could analyze a reduced number of dimensions. This resulted in the biplot below and motivated me to run down the cause of such extreme clustering behavior.\n\n\n\nThe data from the source site is downloadable as two groups. One is the normal Howells crania (2524 crania) and the other is the so-called test set (524 crania). The latter is a collection of crania that were either not whole or had some other deficiency that differentiated them from the normative homo sapien skull. I worked with the normal data set as it had more individuals and as a novice to the world of craniometry I wanted to keep my data as representative of the norm as possible. The Howells set has 82 craniometric measurements in mm, variables for Population in both numeric and string form, Sex, and an individual ID.\n\n\n\n\nAfter generating the first biplot I created the following figure to see if this behavior continued with other PC components:\n\nAt this point it was obvious that PC2 was driving the clustering so I made the variable vector plot below to see if anything jumped out:\n\nThe cluster of highly positive vectors seemed to be the cause of the clustering but there were too many variables in the plot to figure out their names visually. From here I looked into the loadings on PC2 and discovered that the 11 largest were an order of magnitude larger than all the rest. As I was new to craniometrics I didn’t know the conversion from the three letter codes to what the measurements actually meant. I found this publication that described the codes and even mentioned that the variables I was interested in apparently came from a later study. I converted the pdf to .csv tables, read them into R, and added them to the Howells set:\n\nFrom here, I separated the original data set by their position along the PC2 axis in order to grab the two clusters. I then ran a Welch’s t-test to check for significant differences in the means of the different variables. This resulted in some strange results where the t-tests reported means of zero for some variable columns. Looking at the smaller groups data frame I was able to see that there were missing values represented as 0 in those variable columns. The missing data columns were the same variables that had the high loading values in PC space so everything started making sense.\n\nThe missing data was causing a ton of variability. However, there were only 662 individuals with missing values and 1862 individuals with complete data. I believe this is why the variation effect was captured in PC2 as the missing data individuals comprised only roughly 1/5 of the total data.\n\n\n\nSo how to approach this analytically? I could rip apart the R PCA algorithm and take the equations into pure number space but that would remove the visual component that first drew me to this behavior. Instead I decided to plot the PCA results after manipulating a test set of data. I made a data frame of the first 1000 individuals in the Howells data set with only the first 10 craniological measurements. Next I iterated over the frame taking one craniological measurement column and causing 100 then 500 then 900 of the data points in that column to be zero. Then I ran PCAs over these subsets and plotted them. After that, I added another column in, repeated the missing data adjustments, and ran the PCAs. I did this until I saw the clustering move fully into the first PC. The plots are below:\nOne Variable Manipulated:\n  \nTwo Variables:\n  \nThree Variables:\n  \nFour Variables:\n  \nFinally we start making sense of the data. PC components are affected by outlier groups. For each group of plots, the middle plot corresponding to 500, half, of the manipulated columns’ data points are set to 0. But since half the data is affected, the PCA still shows relatively normal clustering behavior as the missing data isn’t a small outlier group, it is now considered a large and thus valid trend in the data space resulting in normative clustering. The 100 and 900 missing data point groups (plots 1 and 3 in each set of 3) create outliers of either missing data or complete data.\nSeeing how the first set of plots (only one manipulated variable column out of 10 total) shows the clustering effect in PC3, the second and third sets (2 and 3 manipulated variables respectively) show clustering in PC2, and the fourth set (4 manipulated columns) show clustering in PC1. This means that the PC that clustering due to missing data set to some arbitrary value occurs in is determined by the number of variables affected by the missing data. For this data set we saw that fewer affected variables meant that this clustering moved into higher PC components.\n\n\n\nOk so what? Why should we care about missing data like this when there are tons of ways to find missing values. To me this opens the door for some fun experiments in PC space. I’ve described the clustering behavior visually here but if we attacked this purely mathematically it might be interesting to see the exact factors that affect which PC shows clustering and the divisions between PC clustering levels. I have a feeling it is a function of n number of variables with k missing values but there could be some fascinating behaviors with specially constructed test sets. I may see what happens with this if, instead of missing data, I “corrupt” some variables with increasing random noise. Regardless, this little foray into craniometric PC analysis has shown that when confronted with interesting behavior, even if that behavior is driven by badly collected data, it is worth investigating its source.\nHere at the end I will freely admit there are easier ways to come to these conclusions, heck there are even easier numerical ways to look for missing data. However, to me at least, there is something beautiful about a visual solution to a numbers based problem."
  },
  {
    "objectID": "project_writeups/kurdistan-sampling-bias.html",
    "href": "project_writeups/kurdistan-sampling-bias.html",
    "title": "A Multi Machine Learning Model Study of Sampling Bias and Environmental Correlation",
    "section": "",
    "text": "Project description: This study acts as the initial proof of concept and variable testing phase for archaeological predictive mapping in the Northern Iraqi section of Kurdistan. The method and results herein are part of a larger study hoping to characterize the geoscientific environmental variables in this area. As archaeological site location data is usually strongly sample biased, we needed to construct a series of test models to see if these biases matched strongly with any of our covariates. Additionally, this gave us the chance to see if the results of our models were consistent with previous publications. We noted various issues with the site locations and environmental covariates and will be addressing these by building a new database that will be outlined in the full publication later this year.\n\n\n\nArchaeological data is typically biased due to uneven sampling. Researchers often collect data from more accessible or visually promising locations, leaving significant gaps. This study tests various predictive models to assess how well they manage these biases and how they correlate with environmental variables.\n\n\n\nThis study employs several statistical and machine learning models:\n\nRandom Forest (RF): Programmed in R via the Biomod2 Library\nGradient Boosting Machine (GBM): Programmed in R via the Biomod2 Library\nMaximum Entropy (MaxEnt): Created via the Maxent.jar GUI as it is written in pure Java and thus much faster over the large area and high number of covariates we used.\n\nAmong these, MaxEnt emerged as the most robust, particularly for archaeological data. It doesn’t require absence data, only presence data, and uses a background point approach to characterize environmental probabilities. This feature is crucial in archaeology, where it’s challenging to identify locations with no historical activity.\nAdditionally, Valavi et al. conducted an extensive evaluation of various species distribution modeling (SDM) algorithms, including machine learning and statistical methods. Their study assessed the performance of 20 different models, identifying Boosted Regression Trees (BRT), MaxEnt, a down-sampled Random Forest (RF), and ensemble models as the top performers. The research highlighted the strengths of MaxEnt for its conservative predictions, which is particularly useful in cases with presence-only data, like in archaeological site modeling. This evaluation serves as a benchmark for selecting appropriate models based on the specific requirements and data characteristics in SDM applications."
  },
  {
    "objectID": "project_writeups/kurdistan-sampling-bias.html#a-multi-machine-learning-model-study-of-sampling-bias-and-environmental-correlation",
    "href": "project_writeups/kurdistan-sampling-bias.html#a-multi-machine-learning-model-study-of-sampling-bias-and-environmental-correlation",
    "title": "A Multi Machine Learning Model Study of Sampling Bias and Environmental Correlation",
    "section": "",
    "text": "Project description: This study acts as the initial proof of concept and variable testing phase for archaeological predictive mapping in the Northern Iraqi section of Kurdistan. The method and results herein are part of a larger study hoping to characterize the geoscientific environmental variables in this area. As archaeological site location data is usually strongly sample biased, we needed to construct a series of test models to see if these biases matched strongly with any of our covariates. Additionally, this gave us the chance to see if the results of our models were consistent with previous publications. We noted various issues with the site locations and environmental covariates and will be addressing these by building a new database that will be outlined in the full publication later this year.\n\n\n\nArchaeological data is typically biased due to uneven sampling. Researchers often collect data from more accessible or visually promising locations, leaving significant gaps. This study tests various predictive models to assess how well they manage these biases and how they correlate with environmental variables.\n\n\n\nThis study employs several statistical and machine learning models:\n\nRandom Forest (RF): Programmed in R via the Biomod2 Library\nGradient Boosting Machine (GBM): Programmed in R via the Biomod2 Library\nMaximum Entropy (MaxEnt): Created via the Maxent.jar GUI as it is written in pure Java and thus much faster over the large area and high number of covariates we used.\n\nAmong these, MaxEnt emerged as the most robust, particularly for archaeological data. It doesn’t require absence data, only presence data, and uses a background point approach to characterize environmental probabilities. This feature is crucial in archaeology, where it’s challenging to identify locations with no historical activity.\nAdditionally, Valavi et al. conducted an extensive evaluation of various species distribution modeling (SDM) algorithms, including machine learning and statistical methods. Their study assessed the performance of 20 different models, identifying Boosted Regression Trees (BRT), MaxEnt, a down-sampled Random Forest (RF), and ensemble models as the top performers. The research highlighted the strengths of MaxEnt for its conservative predictions, which is particularly useful in cases with presence-only data, like in archaeological site modeling. This evaluation serves as a benchmark for selecting appropriate models based on the specific requirements and data characteristics in SDM applications."
  },
  {
    "objectID": "project_writeups/kurdistan-sampling-bias.html#table-1-description-of-variables",
    "href": "project_writeups/kurdistan-sampling-bias.html#table-1-description-of-variables",
    "title": "A Multi Machine Learning Model Study of Sampling Bias and Environmental Correlation",
    "section": "Table 1: Description of Variables",
    "text": "Table 1: Description of Variables\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nAspect\nOrientation of Slope. Calculated from the Slope layer.\n\n\nC_PREC_*\nSum of precipitation over one year for one of the four time periods.\n\n\nC_TEMP_*\nAverage temperature over one year for one of the four time periods.\n\n\nDEM_GLO_25\n25m x 25m Digital Elevation Model (DEM).\n\n\nDistance_to_road\nDistance to an Assyrian road located in the area.\n\n\nDistance_to_water\nDistance to small drainage valleys in the area of interest.\n\n\nGeohydrology\nQualitatively based map of the availability of water, determined by geology.\n\n\nGeology\nGeological map constructed by the Geological Survey of Iraq.\n\n\nGeomorphology\nMap of common geomorphological features, built from ground survey and remote sensing.\n\n\nMRVBF\nMulti-resolution valley bottom flatness, computed from DEM.\n\n\nPREC_sum\nModern average precipitation over one year in the modern age (1972-2000, WorldClim data).\n\n\nSentinel2_NDVI_2021_MedianComposite\nNormalized Difference Vegetation Index from Sentinel2 satellite data.\n\n\nSentinel2_NDWI_2021_MedianComposite\nNormalized Difference Water Index from Sentinel2 satellite data.\n\n\nSlope\nCalculated from the DEM.\n\n\nTEMP_AVG_average\nModern average temperature over one year in the modern age (1972-2000, WorldClim data).\n\n\nTPI\nTopographic Position Index, computed from the DEM, measuring elevation and slope.\n\n\nTWI\nTopographic Wetness Index, computed from the DEM, indicating potential for water accumulation.\n\n\n\nNote: Variables with an asterisk () denote bespoke calculations for each time period.*"
  },
  {
    "objectID": "project_writeups/kurdistan-sampling-bias.html#table-2-description-of-problematic-variables",
    "href": "project_writeups/kurdistan-sampling-bias.html#table-2-description-of-problematic-variables",
    "title": "A Multi Machine Learning Model Study of Sampling Bias and Environmental Correlation",
    "section": "Table 2: Description of Problematic Variables",
    "text": "Table 2: Description of Problematic Variables\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nDistance_cluster_*\nDistance to location of sites for the time period indicated by *.\n\n\nDistance_sites_*\nDistance to location of sites for the age immediately preceding the indicated time period *.\n\n\n\nOur input data was divided into four time periods: Early (EB), Middle (MB), and Late Bronze Age (LB), as well as the Iron Age (IA). Site locations were created by overlaying a map of the polygonal shapes of each site over a raster grid of the study area. Next, we created a point layer with a point in each of the overlapped cells. In this way larger sites would be represented by a larger collection of presence points and have a larger impact on the model. 17 covariates were gathered or calculated. These are shown in Table 1.\nTwo additional layers were calculated for each time period but they were eventually left out of our final calculations. These are shown in Table 2. The reason for leaving these out were that we had intended to measure the effects of site locations from different times on whatever age’s model we were building. However, the Distance_cluster_* caused massive overfitting problems as it is a single layer cost distance model for site location. Any model attempting to predict site locations will eventually discover the utility such a layer and ignore all others. The Distance_sites_* layers had much the same issue, as site locations did not change by large enough margins to be appreciably different from 5 Distance_cluster_*. Once we determined the issues with these layers we decided to drop them from the final data set and only use the preceding 17 covariates. For all models we divided the data into 4 folds for validation tests. We also conducted variable importance tests to measure the relative importance of each covariate. To create absence data for the GBM and RF models, a random selection of points outside of the site locations were found. This selection was equal in size to the amount of presence locations for each age. The MaxEnt model had 20,000 random points selected, some of which could be the same as the presence locations, as MaxEnt attempts to build a probability density distribution of the general background environment.\n\nPredictive Map Results\nThe predictive models constructed in the study—Generalized Boosting Model, Random Forest, and Maximum Entropy—yielded varied results in predicting archaeological site locations in Kurdistan. RF tended to overfit, focusing too narrowly on presence locations and thus limiting its broader applicability. Both MaxEnt and GBM showed a pattern of overemphasis on the “distance to water” covariate, highlighting the environmental feature’s influence but also suggesting potential overfitting and sampling bias. These outputs indicated that while the models could identify high-probability site areas, especially around known features like water sources, there is a need for further refinement to mitigate biases and improve generalizability.\nThe overreliance on the distance to water can be seen in the variable importance metrics for the different models:\n GBM and RF variable importance metrics for the IA model set. The results were consistent across each time period as can be seen in the predictive maps below.\n Maxent variable importance metrics for each time frame.\nDistanct to water covariate visualized in isolation. The darker green an area is the closer it is geographically to a mapped drainage valley. \nBelow are the sresultant predictive maps. Note the similarity in output between the distance to water covariate and the final predictive map.\n \nGBM (Left Column) and RF (Right Column) Predictive Maps in order from EB, MB, LB, IA. Color is correlated with probability green denotes approaching 100% and white means approaching 0%. The values of the color bar can be divided by 1000 to get a percentage.\n\nPredictive maps for MaxEnt. Red denotes approaching 100% probability and Blue means approaching 0% probability.\n\n\nExplanation of Distance to Water Bias\nThe survey method used to locate sites in this region relied on archaeologists following the course of drainage valleys and noting any observed heritage resources. This approach contributed to sampling bias, as it skewed the data towards sites in proximity to water sources, in the case the drainage valleys themselves. As a result, the environmental covariates related to water proximity, such as “distance to water,” became overrepresented in the predictive models. This bias makes it challenging to distinguish whether the observed patterns are due to genuine archaeological factors or are artifacts of the survey method, potentially leading to overfitting and an unbalanced representation of the region’s archaeological landscape.\n\n\nImplications and Future Research Directions\nThe results underscore the importance of addressing both methodological and data-related biases in archaeological modeling. For example, the distance to water feature consistently influenced model outcomes, possibly reflecting both a genuine historical pattern and a bias in data collection methods. Further refinement of the dataset e.g. site density resampling and the inclusion of more diverse environmental layers may lead to the mitigation of these issues.\nThe study’s insights are valuable for refining predictive models in archaeology, ensuring they can better account for biases and provide more accurate site predictions. As part of an ongoing project, the findings will contribute to a more comprehensive geoscientific survey of the region, scheduled for publication later this year.\nThis research not only advances our understanding of bias related to ancient human settlement patterns in Kurdistan but also enhances the methodological toolkit available to archaeologists worldwide."
  },
  {
    "objectID": "project_writeups/chaos-game-3d.html",
    "href": "project_writeups/chaos-game-3d.html",
    "title": "Advanced Data Visualization: A 2D Chaos Game Played in 3 Dimensions",
    "section": "",
    "text": "Project description: The chaos game is one of the classic geometric introductions into chaos theory. I always found it more beautiful than other simple examples like a double pendulum. Creating a Sierpinski triangle is extremely easy but I couldn’t find an implementation in R into 3 dimensions. R is not the best programming language to generate dense 3D fractal point clouds but I thought it would be a fun exercise in optimization to see if I could not only build a 3D version of the 2D chaos game but also get it to render quickly in R. Additionally, I wanted to generalize the 2D chaos game, so rather than the traditional method of subtracting volumes, I wanted to calculate a 2D Sierpinski triangle over each face over a 3D Sierpinski Pyramid. If you want to skip straight to the final result click the link below to see an interactive R shiny app:\n2D Chaos Game in 3D\n\n\nThe Chaos Game is a simple method of generating 2D fractals using a regular polygon. In the triangular case you choose a random point within an equilateral triangle, choose a random vertex, calculate the midpoint between that vertex and the random point, and repeat this using that midpoint as your new random point. After some number of iterations N, your points will begin to show a Sierpinski triangle like the one below:\n\n\nYour browser does not support the video tag. \nIn pseudocode this would look like:\n1. Initialize:\n   - Set the vertices of the triangle: A, B, C\n   - Choose a starting point randomly within the triangle (P)\n\n2. Set the number of iterations (N) for the algorithm\n\n3. Repeat N times:\n   a. Choose one of the vertices (A, B, C) randomly\n   b. Move the current point (P) halfway towards the chosen vertex\n   c. Update the current point (P) to this new position\n\n4. Plot the points (P) as they are generated\n\n5. Display the plot to visualize the Sierpiński triangle\nTry out the app here. [Waffle House Index Simulator](https://edwardarchaeology.shinyapps.io/app_testing/)\n\n\n\nFor my project I wanted to generalize this to 3D which required working with four sided triangular tetrahedrons. The process is quite similar to the 2D case. To start you create a regular triangular tetrahedron, you choose a random point and random vertex, and do the whole midpoint calculation process again. The pseudocode for this is below:\n1. Initialize:\n   - Define the vertices of the tetrahedron: A, B, C, D\n   - Choose a random starting point (P) within the tetrahedron\n\n2. Set the number of iterations (N) for the algorithm\n\n3. Repeat N times:\n   a. Randomly select one of the vertices (A, B, C, D)\n   b. Move the current point (P) halfway towards the chosen vertex\n   c. Update the current point (P) to this new position\n\n4. Plot the points (P) as they are generated\n\n5. Display the plot to visualize the 3D fractal\n\n\n\nI quickly built both of these but found them kind of boring. I decided to go a little off the rails and iteratively play the 2D chaos game in 3D. To do this, I took the tetrahedron and calculated the midpoint along all of its edges. I created four subtetrahedrons using each vertex in combination with the three midpoints of its associated edges. I recursively iterated through this process to some arbitrary depth. This resulted in a massive list of all the vertices for each subtetrahedron the the original tetrahedron. From here I could play the 2D chaos game along each of the four faces of each subtetrahedron. This results in a “hollow” Sierpinski Pyramid with chaotic points delineating its faces. Now, the way I’ve implemented this was just for my own interest in the 2D chaos game being used to create a 3D fractal but the vertex generation procedure could easily be used to generate a triangular mesh over the vertex sets. This could be a fast way to generate 3D fractals as polygonal meshes. A small visual representation of the process and the pseudocode for my method are below:\n\n\nYour browser does not support the video tag. \nFunction Main():\n    // Initial tetrahedron vertices\n    A = (ax, ay, az)\n    B = (bx, by, bz)\n    C = (cx, cy, cz)\n    D = (dx, dy, dz)\n    \n    // Define recursion depth\n    maxDepth = d\n    \n    // List to store vertices of all subtetrahedrons\n    allVertices = []\n\n    // Start the recursive subdivision\n    RecursiveSubdivision(A, B, C, D, 0, maxDepth, allVertices)\n    \n    // Apply 2D Chaos Game on each face of each subtetrahedron\n    For each tetrahedron in allVertices:\n        Apply2DChaosGameOnTetrahedronFaces(tetrahedron)\n    \nEnd Function\n\nFunction RecursiveSubdivision(A, B, C, D, currentDepth, maxDepth, allVertices):\n    // Base case: if maximum depth reached, add vertices to list\n    If currentDepth == maxDepth:\n        allVertices.append((A, B, C, D))\n        Return\n    \n    // Calculate midpoints of all edges\n    M_AB = Midpoint(A, B)\n    M_AC = Midpoint(A, C)\n    M_AD = Midpoint(A, D)\n    M_BC = Midpoint(B, C)\n    M_BD = Midpoint(B, D)\n    M_CD = Midpoint(C, D)\n    \n    // Create four new subtetrahedrons\n    SubTetra1 = (A, M_AB, M_AC, M_AD)\n    SubTetra2 = (B, M_AB, M_BC, M_BD)\n    SubTetra3 = (C, M_AC, M_BC, M_CD)\n    SubTetra4 = (D, M_AD, M_BD, M_CD)\n    \n    // Recursively subdivide each new subtetrahedron\n    RecursiveSubdivision(SubTetra1[1], SubTetra1[2], SubTetra1[3], SubTetra1[4], currentDepth + 1, maxDepth, allVertices)\n    RecursiveSubdivision(SubTetra2[1], SubTetra2[2], SubTetra2[3], SubTetra2[4], currentDepth + 1, maxDepth, allVertices)\n    RecursiveSubdivision(SubTetra3[1], SubTetra3[2], SubTetra3[3], SubTetra3[4], currentDepth + 1, maxDepth, allVertices)\n    RecursiveSubdivision(SubTetra4[1], SubTetra4[2], SubTetra4[3], SubTetra4[4], currentDepth + 1, maxDepth, allVertices)\n\nEnd Function\n\nFunction Midpoint(P1, P2):\n    // Calculate the midpoint between two points in 3D space\n    Return ((P1.x + P2.x) / 2, (P1.y + P2.y) / 2, (P1.z + P2.z) / 2)\nEnd Function\n\nFunction Apply2DChaosGameOnTetrahedronFaces(tetrahedron):\n    // Extract vertices of the tetrahedron\n    (A, B, C, D) = tetrahedron\n    \n    // Apply 2D Chaos Game to each face (triangle) of the tetrahedron\n    Apply2DChaosGame(A, B, C)\n    Apply2DChaosGame(A, B, D)\n    Apply2DChaosGame(A, C, D)\n    Apply2DChaosGame(B, C, D)\nEnd Function\n\nFunction Apply2DChaosGame(A, B, C):\n    // Implement the 2D Chaos Game on a triangular face defined by vertices A, B, and C\n    // (This involves a similar process as the traditional 2D Chaos Game, generating points within the triangle)\n    // Initialize a random point within the triangle\n    P = RandomPointInTriangle(A, B, C)\n    \n    // Define the number of iterations\n    N = 10000  // Arbitrary number for illustration\n    \n    // Iteratively generate points\n    For i from 1 to N:\n        // Randomly select one of the vertices\n        Vertex = RandomChoice([A, B, C])\n        \n        // Move the current point halfway towards the chosen vertex\n        P = Midpoint(P, Vertex)\n        \n        // Store or plot the point as needed\n        PlotPoint(P)  // This function would plot or store the point for visualization\n    \nEnd Function\nThis all resulted in the creation of a shiny app to show off the results of this kind of insane process. Check it out via the link below but be warned, depths 4 and 5 take a long time to render if you have a lot of points:\n2D Chaos Game in 3D"
  },
  {
    "objectID": "project_writeups/chaos-game-3d.html#advanced-data-visualization-a-2d-chaos-game-played-in-3-dimensions",
    "href": "project_writeups/chaos-game-3d.html#advanced-data-visualization-a-2d-chaos-game-played-in-3-dimensions",
    "title": "Advanced Data Visualization: A 2D Chaos Game Played in 3 Dimensions",
    "section": "",
    "text": "Project description: The chaos game is one of the classic geometric introductions into chaos theory. I always found it more beautiful than other simple examples like a double pendulum. Creating a Sierpinski triangle is extremely easy but I couldn’t find an implementation in R into 3 dimensions. R is not the best programming language to generate dense 3D fractal point clouds but I thought it would be a fun exercise in optimization to see if I could not only build a 3D version of the 2D chaos game but also get it to render quickly in R. Additionally, I wanted to generalize the 2D chaos game, so rather than the traditional method of subtracting volumes, I wanted to calculate a 2D Sierpinski triangle over each face over a 3D Sierpinski Pyramid. If you want to skip straight to the final result click the link below to see an interactive R shiny app:\n2D Chaos Game in 3D\n\n\nThe Chaos Game is a simple method of generating 2D fractals using a regular polygon. In the triangular case you choose a random point within an equilateral triangle, choose a random vertex, calculate the midpoint between that vertex and the random point, and repeat this using that midpoint as your new random point. After some number of iterations N, your points will begin to show a Sierpinski triangle like the one below:\n\n\nYour browser does not support the video tag. \nIn pseudocode this would look like:\n1. Initialize:\n   - Set the vertices of the triangle: A, B, C\n   - Choose a starting point randomly within the triangle (P)\n\n2. Set the number of iterations (N) for the algorithm\n\n3. Repeat N times:\n   a. Choose one of the vertices (A, B, C) randomly\n   b. Move the current point (P) halfway towards the chosen vertex\n   c. Update the current point (P) to this new position\n\n4. Plot the points (P) as they are generated\n\n5. Display the plot to visualize the Sierpiński triangle\nTry out the app here. [Waffle House Index Simulator](https://edwardarchaeology.shinyapps.io/app_testing/)\n\n\n\nFor my project I wanted to generalize this to 3D which required working with four sided triangular tetrahedrons. The process is quite similar to the 2D case. To start you create a regular triangular tetrahedron, you choose a random point and random vertex, and do the whole midpoint calculation process again. The pseudocode for this is below:\n1. Initialize:\n   - Define the vertices of the tetrahedron: A, B, C, D\n   - Choose a random starting point (P) within the tetrahedron\n\n2. Set the number of iterations (N) for the algorithm\n\n3. Repeat N times:\n   a. Randomly select one of the vertices (A, B, C, D)\n   b. Move the current point (P) halfway towards the chosen vertex\n   c. Update the current point (P) to this new position\n\n4. Plot the points (P) as they are generated\n\n5. Display the plot to visualize the 3D fractal\n\n\n\nI quickly built both of these but found them kind of boring. I decided to go a little off the rails and iteratively play the 2D chaos game in 3D. To do this, I took the tetrahedron and calculated the midpoint along all of its edges. I created four subtetrahedrons using each vertex in combination with the three midpoints of its associated edges. I recursively iterated through this process to some arbitrary depth. This resulted in a massive list of all the vertices for each subtetrahedron the the original tetrahedron. From here I could play the 2D chaos game along each of the four faces of each subtetrahedron. This results in a “hollow” Sierpinski Pyramid with chaotic points delineating its faces. Now, the way I’ve implemented this was just for my own interest in the 2D chaos game being used to create a 3D fractal but the vertex generation procedure could easily be used to generate a triangular mesh over the vertex sets. This could be a fast way to generate 3D fractals as polygonal meshes. A small visual representation of the process and the pseudocode for my method are below:\n\n\nYour browser does not support the video tag. \nFunction Main():\n    // Initial tetrahedron vertices\n    A = (ax, ay, az)\n    B = (bx, by, bz)\n    C = (cx, cy, cz)\n    D = (dx, dy, dz)\n    \n    // Define recursion depth\n    maxDepth = d\n    \n    // List to store vertices of all subtetrahedrons\n    allVertices = []\n\n    // Start the recursive subdivision\n    RecursiveSubdivision(A, B, C, D, 0, maxDepth, allVertices)\n    \n    // Apply 2D Chaos Game on each face of each subtetrahedron\n    For each tetrahedron in allVertices:\n        Apply2DChaosGameOnTetrahedronFaces(tetrahedron)\n    \nEnd Function\n\nFunction RecursiveSubdivision(A, B, C, D, currentDepth, maxDepth, allVertices):\n    // Base case: if maximum depth reached, add vertices to list\n    If currentDepth == maxDepth:\n        allVertices.append((A, B, C, D))\n        Return\n    \n    // Calculate midpoints of all edges\n    M_AB = Midpoint(A, B)\n    M_AC = Midpoint(A, C)\n    M_AD = Midpoint(A, D)\n    M_BC = Midpoint(B, C)\n    M_BD = Midpoint(B, D)\n    M_CD = Midpoint(C, D)\n    \n    // Create four new subtetrahedrons\n    SubTetra1 = (A, M_AB, M_AC, M_AD)\n    SubTetra2 = (B, M_AB, M_BC, M_BD)\n    SubTetra3 = (C, M_AC, M_BC, M_CD)\n    SubTetra4 = (D, M_AD, M_BD, M_CD)\n    \n    // Recursively subdivide each new subtetrahedron\n    RecursiveSubdivision(SubTetra1[1], SubTetra1[2], SubTetra1[3], SubTetra1[4], currentDepth + 1, maxDepth, allVertices)\n    RecursiveSubdivision(SubTetra2[1], SubTetra2[2], SubTetra2[3], SubTetra2[4], currentDepth + 1, maxDepth, allVertices)\n    RecursiveSubdivision(SubTetra3[1], SubTetra3[2], SubTetra3[3], SubTetra3[4], currentDepth + 1, maxDepth, allVertices)\n    RecursiveSubdivision(SubTetra4[1], SubTetra4[2], SubTetra4[3], SubTetra4[4], currentDepth + 1, maxDepth, allVertices)\n\nEnd Function\n\nFunction Midpoint(P1, P2):\n    // Calculate the midpoint between two points in 3D space\n    Return ((P1.x + P2.x) / 2, (P1.y + P2.y) / 2, (P1.z + P2.z) / 2)\nEnd Function\n\nFunction Apply2DChaosGameOnTetrahedronFaces(tetrahedron):\n    // Extract vertices of the tetrahedron\n    (A, B, C, D) = tetrahedron\n    \n    // Apply 2D Chaos Game to each face (triangle) of the tetrahedron\n    Apply2DChaosGame(A, B, C)\n    Apply2DChaosGame(A, B, D)\n    Apply2DChaosGame(A, C, D)\n    Apply2DChaosGame(B, C, D)\nEnd Function\n\nFunction Apply2DChaosGame(A, B, C):\n    // Implement the 2D Chaos Game on a triangular face defined by vertices A, B, and C\n    // (This involves a similar process as the traditional 2D Chaos Game, generating points within the triangle)\n    // Initialize a random point within the triangle\n    P = RandomPointInTriangle(A, B, C)\n    \n    // Define the number of iterations\n    N = 10000  // Arbitrary number for illustration\n    \n    // Iteratively generate points\n    For i from 1 to N:\n        // Randomly select one of the vertices\n        Vertex = RandomChoice([A, B, C])\n        \n        // Move the current point halfway towards the chosen vertex\n        P = Midpoint(P, Vertex)\n        \n        // Store or plot the point as needed\n        PlotPoint(P)  // This function would plot or store the point for visualization\n    \nEnd Function\nThis all resulted in the creation of a shiny app to show off the results of this kind of insane process. Check it out via the link below but be warned, depths 4 and 5 take a long time to render if you have a lot of points:\n2D Chaos Game in 3D"
  },
  {
    "objectID": "portfolio.html",
    "href": "portfolio.html",
    "title": "Portfolio",
    "section": "",
    "text": "I build practical, end-to-end data and geospatial solutions – from scrapers and ETL pipelines, to interactive apps, dashboards, and research workflows. Below is a selection of projects, reorganized from my original portfolio into two groups: Interactive apps & tools and Research & methods.\n\n\nInteractive apps & tools\n\n\n\n\nTech: Python · LLM · Agents · Docker\n\nCensus LLM Agent\nAn experimental large‑language‑model agent for exploring and querying U.S. Census data, combining natural‑language prompts with structured API calls. Containerized via Docker.\nView code\nLLM Agents Docker Census\n\n\n\n\nTech: Python · Shiny · Geospatial · Satellite Imagery\n\nCoastal Change Detection App\nAn easy, hands-on way for anyonem not just GIS or remote-sensing expert, to explore how coastlines change over time.\nView app\nPython Shiny Environmental Remote sensing\n\n\n\n\n\n\nTech: R · Shiny · Geospatial\n\nWaffle House Index Geospatial App\nA lightning-fast custom geospatial selection and export tool built on Waffle House locations as a proxy for storm impact and service resilience. Supports interactive filtering and one-click data export for planners.\nView app\nShiny Mapping Public safety\n\n\n\nTech: R · Shiny · Remote sensing\n\nDeepwater Horizon Hexadecane App\nAn interactive 3D geospatial app visualizing hexadecane concentrations from the Deepwater Horizon spill. Combines map-based exploration with histograms and temporal controls for scientific storytelling.\nView app\nEnvironmental Remote sensing Visualization\n\n\n\n\nTech: Python · RAG · Local-first AI\n\nGitHub Repository RAG (CosmosDB + Ollama)\nA local-first Retrieval-Augmented Generation system that ingests large collections of GitHub repositories into Cosmos DB (emulator) and uses Ollama for embeddings and chat. Includes helpers to scrape, rank, and clone repos before ingest.\nView code\nRAG Ollama Azure Cosmos\n\n\n\n\n\n\nTech: Python · Desktop app\n\nMonte Carlo Sphere: 2D & 3D π Approximation\nA standalone Python executable that demonstrates Monte Carlo estimation of π in 2D and 3D with interactive visuals. Built to show how numerical methods connect to geometry in an intuitive way.\nView code\nNumerical methods Desktop Education\n\n\n\nTech: Python · APIs · Automation\n\nFBI Crime Data Explorer API Bot\nAn asynchronous query bot that talks to the FBI Crime Data Explorer API, handles large request batches, and outputs structured results for downstream analysis.\nView code\nAPIs Automation Crime data\n\n\n\n\nTech: Tableau · Data viz\n\nWHO Mortality Dashboard\nA Tableau project converting a static WHO mortality analysis into a fully dynamic dashboard. Demonstrates calculated fields, parameter-driven views, and dynamic titles.\nView project\nTableau Health data\n\n\n\n\nTech: Python · QGIS\n\nGeomorphons Feature Processing\nA standalone Python workflow that plugs into QGIS to generate and process geomorphon-based terrain features, making landform classification reproducible and scalable.\nView project\nGeoprocessing Terrain\n\n\n\n\nTech: R · Data viz\n\nChaos Game in 3D\nA 2D chaos game rendered in three dimensions, used as a teaching and exploration tool for probabilistic fractal generation and advanced data visualization.\nView project\nFractals Visualization\n\n\n\n\n\nResearch & methods\n\n\n\n\nTech: Machine learning · Archaeology\n\nSampling Bias & Environmental Correlation in Kurdistan\nA multi-model machine learning study (RF, GBM, MaxEnt) exploring sampling bias and environmental covariates in archaeological predictive mapping for Northern Iraqi Kurdistan.\nView project\nArchaeology SDM Bias\n\n\n\n\nTech: Deep learning · Flood risk\n\nDeep Learning for Flood Evaluation\nAn application of deep learning to evaluate flood extent and risk, combining spatial data with neural networks to improve situational awareness.\nView project\nRemote sensing Flooding\n\n\n\n\nTech: 3D morphometrics\n\nAuto-Landmarking of Human Viscerocranium\nResearch on automating landmark placement on 3D skull scans, using morphometric techniques to support classification and pattern detection in cranial shape.\nView project\n3D Morphometrics\n\n\n\n\nTech: Statistics · Data quality\n\nPCA for Visual Inspection of Missing Data\nUses Principal Component Analysis to visualize and diagnose missing data patterns, helping analysts understand where data gaps might bias downstream models.\nView project\nPCA Data quality\n\n\n\n\nTech: Machine learning · Archaeology\n\nMachine Learning for Archaeological Geographic Data\nAn end-to-end example demonstrating how ML models can support archaeological research using geographic covariates.\nView project\nArchaeology ML\n\n\n\n\nTech: Photogrammetry · 3D\n\nStructure-from-Motion 3D Photogrammetry\nDemonstrates a full SfM workflow to build 3D models from overlapping photographs, used in both archaeology and environmental analysis.\nView project\nPhotogrammetry 3D models"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Have a question or a project in mind? Let’s map the path.\n\n leland.data.consulting@gmail.com\n +1 (540) 397‑7663\n New Orleans, USA\n\n\n\n\n\n\n  \n    \n      Name\n      \n    \n    \n      Email\n      \n    \n  \n\n  \n    \n      Company\n      \n    \n    \n      Timeline\n      \n        Select a timeline\n        ASAP (0–2 weeks)\n        Near‑term (3–6 weeks)\n        Later (6+ weeks)\n      \n    \n  \n\n  \n    \n      Budget (optional)\n      \n        Select a budget\n        &lt;$5K\n        $5K–$15K\n        $15K–$40K\n        $40K+\n      \n    \n  \n\n  \n    \n      Tell me about your project\n      \n    \n  \n\n  \n    Submit"
  },
  {
    "objectID": "contact.html#contact",
    "href": "contact.html#contact",
    "title": "Contact",
    "section": "",
    "text": "Have a question or a project in mind? Let’s map the path.\n\n leland.data.consulting@gmail.com\n +1 (540) 397‑7663\n New Orleans, USA"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello and welcome!\nI’m Edward Leland, a New Orleans–based geospatial & data scientist helping private teams turn messy data and satellite imagery into clear decisions. I build maps, models, and dashboards that answer questions like where to expand, how to reduce risk, and what’s driving performance. Typical engagements include site selection & territory mapping, satellite-based monitoring, market/demand analysis, and executive KPIs and decision dashboards. Delivery is fast, visual, and actionable—so your team can move with confidence.\n\n\n\nScope – clarify decisions & data.\n\nModel – pipeline construction, model development, and final analysis.\n\nDeliver – decision-ready dashboards, applications, and briefs.\n\n\n\n\nPython · R · SQL · Tableau/PowerBI · PostGIS · QGIS · GDAL · Google Earth Engine\nBook a Free Consult"
  },
  {
    "objectID": "about.html#about",
    "href": "about.html#about",
    "title": "About",
    "section": "",
    "text": "Hello and welcome!\nI’m Edward Leland, a New Orleans–based geospatial & data scientist helping private teams turn messy data and satellite imagery into clear decisions. I build maps, models, and dashboards that answer questions like where to expand, how to reduce risk, and what’s driving performance. Typical engagements include site selection & territory mapping, satellite-based monitoring, market/demand analysis, and executive KPIs and decision dashboards. Delivery is fast, visual, and actionable—so your team can move with confidence.\n\n\n\nScope – clarify decisions & data.\n\nModel – pipeline construction, model development, and final analysis.\n\nDeliver – decision-ready dashboards, applications, and briefs.\n\n\n\n\nPython · R · SQL · Tableau/PowerBI · PostGIS · QGIS · GDAL · Google Earth Engine\nBook a Free Consult"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Leland Data Consulting",
    "section": "",
    "text": "Decoding Data. Defining Tomorrow.\n\n\n\n– Data, AI, and geospatial expertise for governments, research centers, and mission-driven organizations.\n\n\n\n\n\n\n\n\nData Science & Business Intelligence\n\nGIS & Location Intelligence\n\nAI and Data Engineering\n\nSatellite and Remote Sensing\n\n\n\n\n\n\n\n\nBook a free consult View services"
  },
  {
    "objectID": "project_writeups/autolandmarking-skulls.html",
    "href": "project_writeups/autolandmarking-skulls.html",
    "title": "Autolandmarking Skulls",
    "section": "",
    "text": "## Automated Landmarking of the Human Skull\nProject description: I was assigned a project to landmark human skulls for an intro to geomorphometrics (GM) class. The assignment was based on manual plotting of landmarks using MeshLab, an outdated 3D mesh analysis software. In the spirit of modernization and leveraging novel methodologies I turned to 3D Slicer, a modern medical analysis software that contains an extension, SlicerMorph, designed to streamline the GM process. What follows is my first steps in attempting to fully automate the landmarking process of the human viscerocranium for this class project as well as the gathering of novel data to test the general applicaiton of these methods.\nNote: All skulls imaged here are not archaeological artefacts. The images here are manipulated stand ins created with the free 3D skull model available here.\n\n\nSemi Automatic vs Automatic Processes.\nFor my class project I was given 11 skulls. 5 male, 5 female, and 1 NA or not assigned. The goal was to attempt to determine the sex of the NA skull and assess researcher error. In MeshLab you would need to manually place landmarks across each skull. This is slow for large amounts of landmarks. Additionally, MeshLab exports files in a .pp format which seems to be an HTML style data storage method. This reduces cross platform usability e.g. importation into R for further analysis.\nAs this process of manual landmark placement is arduous and slow but widely used by arcaheologists, I decided that the first automated landmarking method should resemble the original process, just make it much more efficient. My reasoning here is that archaeologists can be incredibly slow to adapt to new technology, perhaps if it resembles a process which they already follow then they would be less reticent to try this new process. As such, the first step of what I call the semi automatic method required the manual creation of a set of landmark points across traditional osteological points of interest on the skull. I used this paper by Toneva et al. as a basis for choosing the viscerocranium to determine sex as well as to have a systematic landmarking system. I chose a number of landmarks used in their study and manually recreated their position resulting in the following figure:\n\nFrom this point, I leveraged a module named Automated Landmarking through Pointcloud Alignment and Correspondence Analysis or ALPACA. This module is absolutely amazing. It can take in a single set of points and a corresponding model and it will attempt to place a similar set of points across other selected models. Additionally, if your data set is large enough, ALPACA can batch process a few input models and sets of points to increase its accuracy placing landmarks across yoru other data. For my purposes, I input a single model and set of points to ALPACA then went over the autogenerated points and hand corrected any errors. I repeated this process twice over the whole data set to give me three separate sets of manually corrected autogenerated points. This would act as an analogue to researcher error measurements by comparing the accuracy of my hand curation of the auto generated points by measuring the variation in the placement of the landmarks across the three data sets. In additon to simply generating the points, I ran the GPA and PCA over them which allowed me to warp the average shape of the skull via the PC eigenvectors. The figures below show the average skull shape for each ALPACA data set as well as their PC1/2 warps along each axis:\n\nTo extend the idea of automation further, I used another tool, PseudoLMGenerator (PLMG), which can automatically place landmark points across any surface. As the semi auto method was tested on the viscerocranium, when I used PLMG to generate landmarks I deleted every landmark outside this region of interest. The issue with PLMG is that it will create a bespoke landmark set for each input mesh. As such, comparing landmarks between skulls would be impossible. As such, I chose to use PLMG on one skull and then put its output into ALPACA. In the spirit of complete automation I decided to not hand curate the ALPACA points. This had two benefits. The first was that both ALPACA and PLMG have various hyper parameters that can be adjusted to increase their accuracy or change their behavior. While I wanted to investigate these, to do this programatically I would have needed to learn how 3D slicer constructs its data structures in order to extract the relevant data. While not too difficult, I am currently pressed for time with PhD applications and graduation bureaurocracy. This will have to be shelved until I get enough free time to revisit the process. Regardless, my metric of comparason between this automatic method and the previous semi auto method would be to observe if their landmark points produced similar clustering behavior after being run through a Generalized Procrustes Analysis (GPA) and Principle Component Analysis (PCA).\n\n\n\nGPA -&gt; PCA Results\nThe SlicerMorph package contatins a GPA tool. All you need do is point it to the directory input data landmarks and it will run the GPA as well as a PCA. The results from the semi and fully automatic methods were subjected to this process. I exported the results to R for plotting. The first comparison in behavior between the two methods can be seen in the clustering along the first four PC components. Males are blue, Females are Red, and the NA point is in Green. I’ve placed black lines across the 1st PC component comparasons in order to highlight the binary clustering structure.\n\nThe clusters in both cases are largely segregated by Sex. However, there is one male point in the female group and one female point in the male group. This is a relatively small data set so perhaps these outliers would be less suprising in their reciprocity with more input data. The major difference observable here is the NA point in the semi auto data is in the mostly female group while it is in the mostly male group in the auto data.\nA more interesting way to display this data can be created when you add the PC warps to singular biplots. This can be useful when you have significant variance captured by the first two PC components or you just want to illustrate to your audience exactly what the PC vectors represent in real space while also showing the behavior of your data in PC space:\n\n\n\nLDA Confirmation\nTo confirm these visual results mathematically I used the klaR package in R to run a linear discriminate analysis across PC space for each of the 3 semi auto as well as the automatic data sets. For each data set I first generated comparisons between each binary pair of PC components which resulted in the messy plot below:\n\nThis plot is nasty to look at but it gives you an error score for classification across each pair. In the above example the pair with the lowest error score is P2 vs PC5. The second ALPACA Run had the lowest biplot discrimination error rate along PC1 vs PC10, which is shown below with the NA point added in green:\n\nIt is kind of insane to reach into the 10th PC dimension to look for meaningful clustering patterns as PC10 represents such a small amount of variation. So while we can generate plots like these for each data set and we can hope that maximal divisibility will occur across PC1 and PC2 to better justify our results this is not a scientific approach. Rather, I used LDA across every PC dimension to find the multivariate linear model that best described the separation between male and female points. Running this model over NA’s PC data gives us the following posterior results:\n\n\n\nRun\nPosterior Male\nPosterior Female\n\n\n\n\nSemi-Auto 1\n0.59\n0.41\n\n\nSemi-Auto 2\n0.48\n0.52\n\n\nSemi-Auto 3\n0.52\n0.47\n\n\nAutomatic\n0.47\n0.53\n\n\n\nTable: Semi Auto and Automatic LDA Posterior Probabilities\nThese results show that the results of our previous visual inspection were correct in that the semi auto method generally thinks the NA point is a female and the auto method results point to NA being a male. However, in general, the posterior probabilities were extremely close for the categorization of male or female. Perhaps the most firm conclusion we can reach in regards to the sex of the NA individual is that it is indeterminate.\n\n\nVisualizing Measurement Error in Semi Auto Runs\nThe differing results of LDA across the semi auto runs is due to the variation in the placement of the landmarks during the hand curation process. We can simplistically visualize this error by looking at a traditional PC1 vs PC2 biplot. This is done below and the centroid of the three point landmark clusters is shown to better see the distance between each point:\n\nI find this plot unintuitive and kind of ugly. SlicerMorph allows you to visualize the variation in measurement between the different sets of points it uses for a GPA. As such I took the average landmark shape from each ALPACA run, ran a GPA over them, and visualized the variation in measurement between the averages:\n\nThis plot is rotatable in 3D space and you can focus in on areas of interest. In this case each ellipse shows the magnitude and direction of variation for each point. This is a quick visual way to see what data points have the most error, which might need remeasuring, and which are the most accurate.\n\n\nNovel Data Generation and Analysis\nI used an Artec Space Spider Blue-Light 3D Scanner, to create 6 new models for processing. For my own practice I scanned the entirety of the skulls but pre-processed this data in 3D scanner and cut it down to match the sections of skulls used up to this point.\nALPACA can accept multiple input templates to create a more accurate placement of autogenerated landmarks on new data. When used in this way the process is referred to as MALPACA. I used a k-means selection algorithm packaged into slicer morph to find the 3 most representative models from the example data set. I took the hand curated ALPACA results for these and fed them into MALPACA to automatically generate landmark points to the new scans. This process is computationally intensive and would be greatly aided by a newer computer than mine, which is five years old and on its last legs. Creating new landmark points over a large dataset of dense models could take hours, if not days, without proper computational resources. It is a good idea to test this process on a single model as a rough test of how long you might expect MALPACA to run over your entire data set.\nI subjected the results of the MALPACA process over the new data to the same statistical and visualization processes that were used to evaluate the example class data. These results are summarized in the figures below. As I do not know the sex of these individuals, I decided to not perform LDA but rather compare the PCA results between each set of data. The following three figures are: New Data PCA Warps; PCA, Only New Skulls; PCA, All Data (except original NA point),\n  \nVisually the PC eigenvectors cause the average model of the new data to warp in a similar way to the semi and fully automatic output. The PCA results from the new data show a similar division in PCA scores to the example ones in that there are two distinct groups. As I chose the skulls randomly we cannot separate them into male in female groups as of yet but further archival research will hopefully unearth a previous evaluation of the skulls’ sexes. It is important to note that while these skulls do seems to exhibit the same clustering as the example data, as there is only one data point in one of the clusters. This may simply be an outlier skull. Evaluating the normality of the potential outlier with such a small sample size is statistically difficult. However, by removing that skull from the data set and rerunning the GPA/PCA we can scale the remaining points to see if they exhibit strong clustering behavior without the outlier. This is shown below:\n\nThe clustering is ambiguous at best but again this is a small sample size so statistical testing over a larger data set would definitely be beneficial. If we ignore this glaring issue then we could say that the resultant ambiguity points to the “outlier” not really being an outlier. Rather it is a singular sex divided from 5 examples of the other sex. Thus the ambiguous clustering results when it is removed makes sense as there are no longer any large sexual geometric differences between the remaining skulls i.e. they all belong to one sex.\nDetermining which sex these new skulls belong to may be possible by relating their clustering to that seen in the example data. If the new skulls follow the pattern of females to the left of the biplots with males to the right then we would expect these results to mean that the new skulls consist of 5 females and 1 male. However, as this is a separate population from the example data, the division between sexes for out new six skulls could be much smaller meaning that the clustering seen in the example data would not be seen with the new data. Furthermore, perhaps the cluster of 5 new data points (the ones previously assumed to be female) could all be indeterminate within their population. These issues in interpretation are to be expected over such a small data set especially when attempting to compare it to a separate population of humans.\nThough, it is important to address these concerns, these issues will be put to rest once the original evaluation of these skulls’ sexes is found. Regardless of the issues outlined above, the test with new data is a success in that MALPACA was evaluated to work on the kind of skull scans generated by researchers at the University of Tübingen. Even if the interpretations above are incorrect, the process is functional. This means that this workflow can be adopted by the osteology department in order to boost efficiency and accuracy of placing landmarks on 3D scans of human skulls.\n\n\nFurther Considerations\nFrom here it may be useful to see the most important vectors in PC space in order to rate the importance of individual landmarks. This could be used to reduce the size fo the landmark set or perhaps zero in on areas of maximal importance that you need to add more landmarks in. A simple example of how to visualize this data is below:\n\nThis isn’t very useful right now as each vector is split into x,y,z components from the original landmark points. We could reformat the data to be more interpretable but that is a task for another day.\nA fun additional data visualization method is to use the GPA tool from SlicerMorph to create a warp sequence for your PC eigenvectors. You can then go to the Screen Capture Module and create a .gif of the warps. I even set the skull to rotate so you can see the warping over the whole model. This could be useful for data visualization in general or for researchers to get an intimate feel for exactly what the PC eigenvectors represent. You can zoom in on specific parts of the model as well to see minute changes in the structure if you so wish. The results of my first test of this system is shown below:\n\n\n\nConclusion\nBoth the semi and fully automatic methodologies listed here have the ability to save researchers extreme amounts of time compared to the normative MeshLab based process commonly used here at Tübingen. As a novice in this field, I have only begun to scratch the surface of the various tools included within 3D Slicer. I am confident that with access to more time and computing power, I could successfully automate the landmarking procedure for any arbitrarily large set of skull scans. As of now my laptop cannot efficiently engage in more advanced procedures e.g. the registration of hundreds of semi-landmarks across large data sets. However, even the basic methods I have outlined here are more than good enough to become the new standard at our institution, and I can only hope that researchers, smarter and better equipped than I, come up with an even more efficient methodology after this one. The world is changing quickly and we must change with it. The way we do archaeology need not stay static and we should have a questing mindset, always looking for the next improvement to our science."
  },
  {
    "objectID": "project_writeups/geomorphons-standalone.html",
    "href": "project_writeups/geomorphons-standalone.html",
    "title": "Why Standalone?",
    "section": "",
    "text": "Project description: GIS is often constrained to a GUI. While this can provide a quick way to collate complex collections of features, it can become cumbersome when you need to perform the same task rapidly over large repositories of data. In order to solve this, it is possible to develop workflows in standalone Python that use QGIS tools. When used in combination with other processing libraries this gives researchers the ability to run complex analysis over lots of data quickly and efficiently without worrying about the RAM limitations inherent to the QGIS GUI. Additionally QGIS is a great set of tools but it falls prey to processes getting stuck in infinite hanging loops due to different plugins not communicating well with each other. This frequenctly occurs when using any form of batch processing in the QGIS GUI. Another fun extention of the Python standalone model is that this system can be run within a Jupyter Notebook allowing for analysts to seamlessly combine .md style documentation with their code. This provides an accessible document useful for sharing with other researchers or for teaching those unfamiliar with your workflow.\n\n\nIn recent years, geophysicists have constructed algorithms for processing digital elevation maps in order to categorize their features into the 10 most common geomorphometric features. QGIS offers an exceptionally quick version of this via its use of the GRASS r.geomorphon processing method. One easily seen use of this is for the following archaeological application. In the satellite image below, nothing stands out:\n\nYet if we use r.geomorphon on a DEM of the area (in this case constructed via processing tiled LIDAR point clouds into one continuous surface) we see an interesting feature arise on the mountain top:\n\nExtracting just the ridge features from the data set gives:\n\nThe circular ridge layers highlighted by the red circle define a site of archaeological interest currently under study. This technique could be extended upon by running it over large DEM’s and using a machine learning classifier to extract tiles where similar features are found. This could lead to an extremely quick method to discovering this type of hilltop archaeological site.\nNow, when trying to perform this calculation over a series of raster images I kept having to run each image individually as r.geomorphon would get hung up after batch processing sometimes just two raster files. I could have run everything from the command line but I wanted a more elegan solution.\n\n\n\nGetting QGIS to work with an IDE is pretty simple especially if you are looking to run it in a Jupyter notebook:\n\nNavigate to and click on the OSGeo4W.bat file.\n\nThe default location of this is the C:\\OSGeo4W directory.\n\nIn the OSGeo4W terminal that opens, install Jupyter with the command below:\npython -m pip install jupyter\nIn a file explorer window, navigate to the python-qgis-ltr.bat file and copy its path.\n\nOn my machine this was: C:\\OSGeo4W\\bin\\python-qgis-ltr.bat\nNote: if you are not using the long-term release (LTR) version of QGIS, the file will be called python-qgis.bat.\n\nIn the OSGeo4W terminal, set your working directory to wherever you want to open the Jupyter file explorer.\nRun the command below and you should have a Jupyter window pop up in your default browser:\nC:\\OSGeo4W\\bin\\python-qgis-ltr.bat -m notebook"
  },
  {
    "objectID": "project_writeups/geomorphons-standalone.html#why-standalone",
    "href": "project_writeups/geomorphons-standalone.html#why-standalone",
    "title": "Why Standalone?",
    "section": "",
    "text": "Project description: GIS is often constrained to a GUI. While this can provide a quick way to collate complex collections of features, it can become cumbersome when you need to perform the same task rapidly over large repositories of data. In order to solve this, it is possible to develop workflows in standalone Python that use QGIS tools. When used in combination with other processing libraries this gives researchers the ability to run complex analysis over lots of data quickly and efficiently without worrying about the RAM limitations inherent to the QGIS GUI. Additionally QGIS is a great set of tools but it falls prey to processes getting stuck in infinite hanging loops due to different plugins not communicating well with each other. This frequenctly occurs when using any form of batch processing in the QGIS GUI. Another fun extention of the Python standalone model is that this system can be run within a Jupyter Notebook allowing for analysts to seamlessly combine .md style documentation with their code. This provides an accessible document useful for sharing with other researchers or for teaching those unfamiliar with your workflow.\n\n\nIn recent years, geophysicists have constructed algorithms for processing digital elevation maps in order to categorize their features into the 10 most common geomorphometric features. QGIS offers an exceptionally quick version of this via its use of the GRASS r.geomorphon processing method. One easily seen use of this is for the following archaeological application. In the satellite image below, nothing stands out:\n\nYet if we use r.geomorphon on a DEM of the area (in this case constructed via processing tiled LIDAR point clouds into one continuous surface) we see an interesting feature arise on the mountain top:\n\nExtracting just the ridge features from the data set gives:\n\nThe circular ridge layers highlighted by the red circle define a site of archaeological interest currently under study. This technique could be extended upon by running it over large DEM’s and using a machine learning classifier to extract tiles where similar features are found. This could lead to an extremely quick method to discovering this type of hilltop archaeological site.\nNow, when trying to perform this calculation over a series of raster images I kept having to run each image individually as r.geomorphon would get hung up after batch processing sometimes just two raster files. I could have run everything from the command line but I wanted a more elegan solution.\n\n\n\nGetting QGIS to work with an IDE is pretty simple especially if you are looking to run it in a Jupyter notebook:\n\nNavigate to and click on the OSGeo4W.bat file.\n\nThe default location of this is the C:\\OSGeo4W directory.\n\nIn the OSGeo4W terminal that opens, install Jupyter with the command below:\npython -m pip install jupyter\nIn a file explorer window, navigate to the python-qgis-ltr.bat file and copy its path.\n\nOn my machine this was: C:\\OSGeo4W\\bin\\python-qgis-ltr.bat\nNote: if you are not using the long-term release (LTR) version of QGIS, the file will be called python-qgis.bat.\n\nIn the OSGeo4W terminal, set your working directory to wherever you want to open the Jupyter file explorer.\nRun the command below and you should have a Jupyter window pop up in your default browser:\nC:\\OSGeo4W\\bin\\python-qgis-ltr.bat -m notebook"
  },
  {
    "objectID": "project_writeups/ml-archaeology-geo.html",
    "href": "project_writeups/ml-archaeology-geo.html",
    "title": "Machine Learning with Geographic Data",
    "section": "",
    "text": "Project description: Geographic data is usually manipulated via GIS systems e.g. ArcGIS/QGIS. While these technologies provide a relatively user-friendly GUI for visual manipulation of vectors, rasters, and images, analysis is limited to out of the box algorithms and methodologies. By moving GIS data into a pure programming environment we are able to use exponentially more investigatory tools including cutting edge machine learning techniques. Below is a brief overview of one of the studies I’ve conducted using ML for GIS applications.\n\n\nWhen trying to predict site locations, archaeologists can only rely on presence only data. That is, they know where heritage resources have been discovered but they cannot be 100% sure that at a randomly chosen point within their study area there doesn’t exist an, as of yet, undiscovered archaeological site. This presents a problem for those familiar with machine learning. Common techniques like GLM, Random Forest, XGBoost, etc. require that their input be divided into categories. For example, for simple image classification you might submit a set of pictures of dogs and one of cats for your model to learn the differences between the two. However, we only have one set of data, the points where we know archaeological sites exist.\n\n\n\nMaximum Entropy, or MaxEnt, is a presence only machine learning algorithm commonly used by ecologists to model species dispersion. This technique lends itself well to the problem of archaeological site prediction as both ecologists and archaeologists usually work with datasets that indicate only where their object of study was found. MaxEnt works by first analyzing different environmental variables over the entirety of the study area. From these, it builds a generalized background probability density distribution. MaxEnt then calculates the distribution of environmental variable values at the presence locations. From here MaxEnt iterates, estimating various probability density distributions for the study area. These estimates are constructed with the goal of maximizing their closeness to the generalized distribution while also having similar environmental values in the presence locations.\nIf this is a bit confusing, I apologize. An in depth explanation of MaxEnt is its own academic article (in fact there are a bunch written on the subject) but a great summary can be found here.\n\n\n\nA Maximum Entropy derived occurrence map for probable human habitation sites during the Bronze Age.\n\nThis map shows the most probable areas for human habitation in Red. From this we can draw conclusions about ancient human settlement behavior! There are various other analyses that can help to provide further insight into the past. PCA can help determine what covariates may need to be dropped in order to reduce correlation bias errors. Variable importance within the model itself can help archaeologists understand what environmental factors most affected ancient peoples’ habitation decisions. We even looked into the distribution of habitation patterns across different epochs as well as the distributions by site types e.g. settlement, burial ground, etc. I’d love to go further in depth on what we discovered but this page is already getting a little long for an overview, so feel free to contact me for more details if this piques your interest."
  },
  {
    "objectID": "project_writeups/ml-archaeology-geo.html#machine-learning-with-geographic-data",
    "href": "project_writeups/ml-archaeology-geo.html#machine-learning-with-geographic-data",
    "title": "Machine Learning with Geographic Data",
    "section": "",
    "text": "Project description: Geographic data is usually manipulated via GIS systems e.g. ArcGIS/QGIS. While these technologies provide a relatively user-friendly GUI for visual manipulation of vectors, rasters, and images, analysis is limited to out of the box algorithms and methodologies. By moving GIS data into a pure programming environment we are able to use exponentially more investigatory tools including cutting edge machine learning techniques. Below is a brief overview of one of the studies I’ve conducted using ML for GIS applications.\n\n\nWhen trying to predict site locations, archaeologists can only rely on presence only data. That is, they know where heritage resources have been discovered but they cannot be 100% sure that at a randomly chosen point within their study area there doesn’t exist an, as of yet, undiscovered archaeological site. This presents a problem for those familiar with machine learning. Common techniques like GLM, Random Forest, XGBoost, etc. require that their input be divided into categories. For example, for simple image classification you might submit a set of pictures of dogs and one of cats for your model to learn the differences between the two. However, we only have one set of data, the points where we know archaeological sites exist.\n\n\n\nMaximum Entropy, or MaxEnt, is a presence only machine learning algorithm commonly used by ecologists to model species dispersion. This technique lends itself well to the problem of archaeological site prediction as both ecologists and archaeologists usually work with datasets that indicate only where their object of study was found. MaxEnt works by first analyzing different environmental variables over the entirety of the study area. From these, it builds a generalized background probability density distribution. MaxEnt then calculates the distribution of environmental variable values at the presence locations. From here MaxEnt iterates, estimating various probability density distributions for the study area. These estimates are constructed with the goal of maximizing their closeness to the generalized distribution while also having similar environmental values in the presence locations.\nIf this is a bit confusing, I apologize. An in depth explanation of MaxEnt is its own academic article (in fact there are a bunch written on the subject) but a great summary can be found here.\n\n\n\nA Maximum Entropy derived occurrence map for probable human habitation sites during the Bronze Age.\n\nThis map shows the most probable areas for human habitation in Red. From this we can draw conclusions about ancient human settlement behavior! There are various other analyses that can help to provide further insight into the past. PCA can help determine what covariates may need to be dropped in order to reduce correlation bias errors. Variable importance within the model itself can help archaeologists understand what environmental factors most affected ancient peoples’ habitation decisions. We even looked into the distribution of habitation patterns across different epochs as well as the distributions by site types e.g. settlement, burial ground, etc. I’d love to go further in depth on what we discovered but this page is already getting a little long for an overview, so feel free to contact me for more details if this piques your interest."
  },
  {
    "objectID": "project_writeups/photogrammetry-3d.html",
    "href": "project_writeups/photogrammetry-3d.html",
    "title": "SFM 3D Model Building",
    "section": "",
    "text": "Project description: Photogrammetric model construction of various fossils. Different subjects were chosen for different levels of model building difficulty.\n\n\nThe following number of photos were captured for each model:\n\nAmmonite Wall: 228 photos\nMarine Skeletal Fossil: 230 photos\nSingle Ammonite: 92 photos\nHearth: 164 photos\nSecond Ammonite Wall: 247 photos\n\nTotal data collection time was approximately 3 hours. Models were built in Agisoft on the lowest possible settings to investigate whether or not presentation/publication worthy models could be built quickly with low computational requirements. The results were great for this purpose and show that even for relatively difficult subjects, detailed models could be constructed.\n\n\n\nAmmonite Wall \nFlat Fossil \nSingle Ammonite \nPrehistoric Hearth \nSecond Ammonite Wall \n\n\n\nFor further details check out this short paper describing the model creation process. Model Building."
  },
  {
    "objectID": "project_writeups/photogrammetry-3d.html#sfm-3d-model-building",
    "href": "project_writeups/photogrammetry-3d.html#sfm-3d-model-building",
    "title": "SFM 3D Model Building",
    "section": "",
    "text": "Project description: Photogrammetric model construction of various fossils. Different subjects were chosen for different levels of model building difficulty.\n\n\nThe following number of photos were captured for each model:\n\nAmmonite Wall: 228 photos\nMarine Skeletal Fossil: 230 photos\nSingle Ammonite: 92 photos\nHearth: 164 photos\nSecond Ammonite Wall: 247 photos\n\nTotal data collection time was approximately 3 hours. Models were built in Agisoft on the lowest possible settings to investigate whether or not presentation/publication worthy models could be built quickly with low computational requirements. The results were great for this purpose and show that even for relatively difficult subjects, detailed models could be constructed.\n\n\n\nAmmonite Wall \nFlat Fossil \nSingle Ammonite \nPrehistoric Hearth \nSecond Ammonite Wall \n\n\n\nFor further details check out this short paper describing the model creation process. Model Building."
  },
  {
    "objectID": "project_writeups/simple-spatial-shiny.html",
    "href": "project_writeups/simple-spatial-shiny.html",
    "title": "Simple Spatial Shiny App",
    "section": "",
    "text": "This writeup describes a small R Shiny application used as a proof of concept for sharing geospatial data interactively without requiring users to install desktop GIS software.\nThe app:\n\nScrapes current Waffle House locations from the public website.\nPlots them on an interactive map.\nLets the user draw a synthetic disaster area with configurable radii.\nPerforms real‑time CRS transformations and distance calculations to render the impact zone correctly.\n\n\nDespite its simplicity, the app demonstrates:\n\nReactive user input bound to spatial calculations.\nAutomated data refresh via web scraping.\nHow quickly an audience‑friendly, shareable GIS interface can be built with Shiny.\n\nDeployed version:\nhttps://edwardarchaeology.shinyapps.io/app_testing/"
  },
  {
    "objectID": "project_writeups/simple-spatial-shiny.html#a-minimal-r-shiny-app-with-spatial-data",
    "href": "project_writeups/simple-spatial-shiny.html#a-minimal-r-shiny-app-with-spatial-data",
    "title": "Simple Spatial Shiny App",
    "section": "",
    "text": "This writeup describes a small R Shiny application used as a proof of concept for sharing geospatial data interactively without requiring users to install desktop GIS software.\nThe app:\n\nScrapes current Waffle House locations from the public website.\nPlots them on an interactive map.\nLets the user draw a synthetic disaster area with configurable radii.\nPerforms real‑time CRS transformations and distance calculations to render the impact zone correctly.\n\n\nDespite its simplicity, the app demonstrates:\n\nReactive user input bound to spatial calculations.\nAutomated data refresh via web scraping.\nHow quickly an audience‑friendly, shareable GIS interface can be built with Shiny.\n\nDeployed version:\nhttps://edwardarchaeology.shinyapps.io/app_testing/"
  },
  {
    "objectID": "project_writeups/who-mortality-dashboard.html",
    "href": "project_writeups/who-mortality-dashboard.html",
    "title": "WHO Mortality Dashboard",
    "section": "",
    "text": "image\n\n\nProject description: With Tableau being all the rage in data informatics these days, I can’t hide behind my mastery of ggplot and matplotlib. As wonderful as those tools are for bespoke data visualization, 9/10 times in industry you just need some nice charts for a meeting with surface level user interaction. As such, I’ve joined the masses and, in addition to the arcane skills of bespoke script based graphics, I’m now proficient in Tableau. I found a fascinating data set from the World Health Organization (WHO) that consists of all the major causes of death across the globe for six different years between 2000 and 2021 and managed to make a dashboard that integrates some really cool functionalities of Tableau without being too overwhelming.\nLink to the Tableau Dashboard\n\n\n\n\n\nimage\n\n\nAll the data used in this project can be found here. The WHO provides six excel workbooks of tracked causes of death for each country they monitor. Each workbook corresponds to a different year and the years available are 2000, 2010, 2015, 2019, 2020, 2021. Each workbook is divided into 9 sheets with the first being a notes page, the second a collection of all causes of deaths across all ages, and the rest are causes of death broken down by age categories. As I was interested in the top causes of death per country I used the second page of each workbook as my data source.\nEach second page was a massive, 661x192, and formatted to be human readable with a lot of white space bullet point style organization. Additionally, these pages used conditional formatting to highlight countries with differing levels of statistical verification of their reported numbers. Green was high completeness/quality and transitioned through yellow to orange then finally red as the data became less complete/verified. As such, I chose to focus on the verified countries as any interesting visual trends seen in the final Tableau dashboard would have more statistical weight and meaning behind them.\n\n\n\nI use Tableau Public which doesn’t come with Tableau Prep, the GUI data manipulation tool. As such, all of my pre-processing was done in R. This required importing each excel workbook into R, grabbing the second sheet with the data we want, extracting only the relevant data, and doing a bunch of formatting to change the human readable spreadsheet into a CSV that Tableau accepted.\nlibrary(tidyverse)  # For data manipulation functions (dplyr, tidyr, etc.)\nlibrary(readxl)     # For reading Excel files\n\n# Function to read the \"All ages\" sheet from the given Excel file and drop unnecessary rows\nread_sheet &lt;- function(file_name) {\n  sheet &lt;- read_excel(file_name, sheet = \"All ages\")[-1:-5,]\n  return(sheet)\n}\n\n# Function to clean the \"All ages\" sheet data\nclean_sheet &lt;- function(sheet) {\n  \n  all_ages_sheet &lt;- sheet\n  \n  # Filter out rows where the first column is either \"Persons\", NA, or \"Sex\"\n  # Eliminates summary, sex divided, and empty rows\n  all_ages_sheet &lt;- all_ages_sheet %&gt;%\n    filter(all_ages_sheet[[1]] == \"Persons\" | is.na(all_ages_sheet[[1]]) | all_ages_sheet[[1]] == \"Sex\")\n  \n  # Remove the first 5 bullet point formatted info columns\n  all_ages_sheet &lt;- all_ages_sheet[,-1:-5]\n  \n  # Grab non-sub divided cause of death info and add in placeholder data \n  # This avoids dropping the column in a later step\n  causes &lt;- all_ages_sheet[,1]\n  causes[1:7,] &lt;- \"Placeholder\"\n  \n  # Keep columns that represent verified country data based on specific criteria\n  # Hidden conditional formatting numbers in the row under 3 letter country codes\n  verified_country_data &lt;- all_ages_sheet %&gt;% select(where(~ !is.na(.[3]) && .[3] &lt; 2))\n  \n  # Combine the causes of death with the verified country data\n  verified_N_causes &lt;- cbind(causes, verified_country_data)\n  \n  # Remove rows that contain patterns like lowercase letters followed by a period\n  # Also remove rows with NA in the 6th column\n  df_initial &lt;- verified_N_causes[!grepl(\"^[a-z]\\\\.$\", verified_N_causes$...6) & !is.na(verified_N_causes$...6), ]\n  \n  # Remove the first 4 rows\n  df_cleaned &lt;- df_initial[-c(1:4), ]\n  \n  # Set the first row as column names and remove the first 3 rows of data\n  colnames(df_cleaned) &lt;- c(\"Cause of Death\", as.character(df_initial[1, -1]))\n  df_cleaned &lt;- df_cleaned[-1:-3,]\n  \n  return(df_cleaned)  # Return the cleaned dataset\n}\n\n# Function to build a dataset for a specific year\nbuild_year_df &lt;- function(df_cleaned, Year) {\n  \n  # Extract the list of countries from the cleaned data, skipping the \"Cause of Death\" column\n  countries &lt;- colnames(df_cleaned)[-1:-2]  # Ignore the first two columns\n  \n  # Initialize an empty list and dataframe to store the top causes of death for each country\n  top_causes_per_country &lt;- list()\n  test &lt;- data.frame()\n  \n  # For each country, find the top 5 causes of death\n  for (country in countries) {\n    # Sort the data by the death numbers for the country in descending order\n    df_sorted &lt;- df_cleaned %&gt;%\n      arrange(desc(as.numeric(.data[[country]]))) %&gt;%\n      select(\"Cause of Death\", all_of(country)) %&gt;%\n      head(5)  # Select the top 5 causes of death for the country\n    \n    # Add the country name and year to the sorted data\n    df_sorted$Country &lt;- country\n    df_sorted$Year &lt;- Year\n    \n    # Rename the column containing the death numbers to \"Numbers\"\n    names(df_sorted)[names(df_sorted) == country] &lt;- \"Numbers\"\n    \n    # Append the sorted data to the result dataframe\n    test &lt;- rbind(test, df_sorted)\n  }\n  \n  # Convert the results to a dataframe and return it\n  top_causes_df &lt;- as.data.frame(test)\n}\n\n# Function to extract the second year from a filename string (if it contains multiple years)\nget_second_year &lt;- function(string) {\n  # Find all four-digit numbers (representing years) in the filename\n  matches &lt;- regmatches(string, gregexpr(\"\\\\d{4}\", string))\n  \n  # Return the second year if it exists, otherwise return NA\n  if (length(matches[[1]]) &gt;= 2) {\n    return(matches[[1]][2])\n  } else {\n    return(NA)\n  }\n}\n\n# Set the working directory to the folder containing the Excel files\npath &lt;- \"D:/Data_Projects/Tableau/WHO_Death/Data/Raw_Year_xl\"\nsetwd(path)\n\n# Get a list of all files in the directory\nrlist &lt;- list.files(\".\")\n\n# Apply the get_second_year function to each file to extract the year information\nYears &lt;- sapply(rlist, get_second_year)\n\n# Initialize an empty dataframe to store all the data from multiple years\nall_data &lt;- data.frame()\n\n# Initialize a counter to track the current file\ncount &lt;- 1\n\n# Loop through each year and process the corresponding Excel file\nfor (year in Years) {\n  # Read, clean, and build the dataset for the current year\n  df_year &lt;- build_year_df(clean_sheet(read_sheet(rlist[count])), year)\n  \n  # Append the cleaned and processed data to the main dataframe\n  all_data &lt;- rbind(all_data, df_year)\n  \n  # Increment the counter to move to the next file\n  count &lt;- count + 1\n}\n\n# Write the combined data for all years and countries to a CSV file\nwrite.csv(all_data, file = \"Top_5_All_Years.csv\")\nThe final CSV looked like this:\n\n\n\nimage\n\n\n\n\n\nAs we have data separated by space, time, and category, two types of charts immediately come to mind. Firstly, a world map to cover the spatial dimension and secondly a stacked bar chart of causes of death per year which takes care of time and category. I wanted the user to be able to select multiple countries and be able to visually understand trends in major causes of death as well as being able to select causes of death to be highlighted across the data. This posed the perfect challenge as it requires some tinkering across multiple chart and data types as well as actions, expressions, filters, parameters, and calculated fields. Thus, a relatively simple dashboard can have some interesting behaviors that users might not often get to see.\n\n\n\n\n\n\nimage\n\n\nTableau is awesome for auto-generating simple map based charts. Simply butting the country column from the CSV data source into the details section created a monocolor map. I wanted to add color in a meaningful way without overwhelming the dashboard user. To do so I decided that instead of using some kind of multi color scheme to represent multiple causes of death per country over the six years, I would highlight each country via its top cause of death for a year selected by the user. Making a Year Selected parameter that the user could change was easy but finding the top cause of death for each year for each country was a little tricky. Tableau uses its bespoke Tableau Calculation Language (TCL) for internal data wrangling and it definitely has some quirks. As such I usually put in a couple layers of redundancy to catch errors before they happen. First, I created a calculated field that grabbed the cause of death numbers for a chosen value of the Selected Year parameter:\n\n\n\nimage\n\n\nNext I used the following expression:\n\n\n\nimage\n\n\nThe first line iterates, grabbing unique combinations of Country, COD, and Selected Year and returns their associated number of death values. The second line is basically the same thing but finds the maximum total number of deaths for each country across all causes of death. It does this by first calculating the sum of deaths for each combination of Country and Cause of Death (like the first expression) and then taking the maximum of those sums for each country. By comparing these two expressions we return true when a unique combination of Country, COD, and Selected Year has a sum equal to the maximum sum for that country. This gives us the top cause of death per country for a given year.\nFrom there, I used this boolean to drive map coloring and tooltips, so clicking a country both filters the bar chart and highlights that country’s top COD.\n\n\n\nThe second major view is a stacked bar chart of causes of death by year. It uses the same Top_5_All_Years.csv data source, filtered down to the top causes per country:\n\nBars encode the total number of deaths.\nColors encode the cause of death.\nFilters let the user focus on selected countries and causes.\n\nWith parameter actions and highlight actions, clicking a bar segment highlights that cause across all selected countries and years, giving a quick sense of how a particular COD behaves globally.\n\n\n\nA small but important UX touch is the dynamic guidance in chart titles. When nothing is selected, titles instruct the user to click a country or bar. Once a country is selected, the titles update to show the selection. Clearing the selection restores the helper text instead of leaving blank or confusing views.\nTo do this, I used a Country Selected parameter and a worksheet action that resets the parameter when the selection is cleared. The dashboard action looks like:\n\n\n\nimage\n\n\nIt’s a small detail, but it makes the dashboard much more approachable. Tableau makes “good enough” visuals easy; polishing the UX requires a bit more effort and an understanding of how its calculation language and actions work together.\nFor me, this project was as much about sharpening my Tableau skills (parameters, actions, and TCL quirks) as it was about the mortality data itself.\nLink to the Tableau Dashboard"
  },
  {
    "objectID": "project_writeups/who-mortality-dashboard.html#tableau-showcase-from-fixed-expressions-to-dynamic-titles",
    "href": "project_writeups/who-mortality-dashboard.html#tableau-showcase-from-fixed-expressions-to-dynamic-titles",
    "title": "WHO Mortality Dashboard",
    "section": "",
    "text": "image\n\n\nProject description: With Tableau being all the rage in data informatics these days, I can’t hide behind my mastery of ggplot and matplotlib. As wonderful as those tools are for bespoke data visualization, 9/10 times in industry you just need some nice charts for a meeting with surface level user interaction. As such, I’ve joined the masses and, in addition to the arcane skills of bespoke script based graphics, I’m now proficient in Tableau. I found a fascinating data set from the World Health Organization (WHO) that consists of all the major causes of death across the globe for six different years between 2000 and 2021 and managed to make a dashboard that integrates some really cool functionalities of Tableau without being too overwhelming.\nLink to the Tableau Dashboard\n\n\n\n\n\nimage\n\n\nAll the data used in this project can be found here. The WHO provides six excel workbooks of tracked causes of death for each country they monitor. Each workbook corresponds to a different year and the years available are 2000, 2010, 2015, 2019, 2020, 2021. Each workbook is divided into 9 sheets with the first being a notes page, the second a collection of all causes of deaths across all ages, and the rest are causes of death broken down by age categories. As I was interested in the top causes of death per country I used the second page of each workbook as my data source.\nEach second page was a massive, 661x192, and formatted to be human readable with a lot of white space bullet point style organization. Additionally, these pages used conditional formatting to highlight countries with differing levels of statistical verification of their reported numbers. Green was high completeness/quality and transitioned through yellow to orange then finally red as the data became less complete/verified. As such, I chose to focus on the verified countries as any interesting visual trends seen in the final Tableau dashboard would have more statistical weight and meaning behind them.\n\n\n\nI use Tableau Public which doesn’t come with Tableau Prep, the GUI data manipulation tool. As such, all of my pre-processing was done in R. This required importing each excel workbook into R, grabbing the second sheet with the data we want, extracting only the relevant data, and doing a bunch of formatting to change the human readable spreadsheet into a CSV that Tableau accepted.\nlibrary(tidyverse)  # For data manipulation functions (dplyr, tidyr, etc.)\nlibrary(readxl)     # For reading Excel files\n\n# Function to read the \"All ages\" sheet from the given Excel file and drop unnecessary rows\nread_sheet &lt;- function(file_name) {\n  sheet &lt;- read_excel(file_name, sheet = \"All ages\")[-1:-5,]\n  return(sheet)\n}\n\n# Function to clean the \"All ages\" sheet data\nclean_sheet &lt;- function(sheet) {\n  \n  all_ages_sheet &lt;- sheet\n  \n  # Filter out rows where the first column is either \"Persons\", NA, or \"Sex\"\n  # Eliminates summary, sex divided, and empty rows\n  all_ages_sheet &lt;- all_ages_sheet %&gt;%\n    filter(all_ages_sheet[[1]] == \"Persons\" | is.na(all_ages_sheet[[1]]) | all_ages_sheet[[1]] == \"Sex\")\n  \n  # Remove the first 5 bullet point formatted info columns\n  all_ages_sheet &lt;- all_ages_sheet[,-1:-5]\n  \n  # Grab non-sub divided cause of death info and add in placeholder data \n  # This avoids dropping the column in a later step\n  causes &lt;- all_ages_sheet[,1]\n  causes[1:7,] &lt;- \"Placeholder\"\n  \n  # Keep columns that represent verified country data based on specific criteria\n  # Hidden conditional formatting numbers in the row under 3 letter country codes\n  verified_country_data &lt;- all_ages_sheet %&gt;% select(where(~ !is.na(.[3]) && .[3] &lt; 2))\n  \n  # Combine the causes of death with the verified country data\n  verified_N_causes &lt;- cbind(causes, verified_country_data)\n  \n  # Remove rows that contain patterns like lowercase letters followed by a period\n  # Also remove rows with NA in the 6th column\n  df_initial &lt;- verified_N_causes[!grepl(\"^[a-z]\\\\.$\", verified_N_causes$...6) & !is.na(verified_N_causes$...6), ]\n  \n  # Remove the first 4 rows\n  df_cleaned &lt;- df_initial[-c(1:4), ]\n  \n  # Set the first row as column names and remove the first 3 rows of data\n  colnames(df_cleaned) &lt;- c(\"Cause of Death\", as.character(df_initial[1, -1]))\n  df_cleaned &lt;- df_cleaned[-1:-3,]\n  \n  return(df_cleaned)  # Return the cleaned dataset\n}\n\n# Function to build a dataset for a specific year\nbuild_year_df &lt;- function(df_cleaned, Year) {\n  \n  # Extract the list of countries from the cleaned data, skipping the \"Cause of Death\" column\n  countries &lt;- colnames(df_cleaned)[-1:-2]  # Ignore the first two columns\n  \n  # Initialize an empty list and dataframe to store the top causes of death for each country\n  top_causes_per_country &lt;- list()\n  test &lt;- data.frame()\n  \n  # For each country, find the top 5 causes of death\n  for (country in countries) {\n    # Sort the data by the death numbers for the country in descending order\n    df_sorted &lt;- df_cleaned %&gt;%\n      arrange(desc(as.numeric(.data[[country]]))) %&gt;%\n      select(\"Cause of Death\", all_of(country)) %&gt;%\n      head(5)  # Select the top 5 causes of death for the country\n    \n    # Add the country name and year to the sorted data\n    df_sorted$Country &lt;- country\n    df_sorted$Year &lt;- Year\n    \n    # Rename the column containing the death numbers to \"Numbers\"\n    names(df_sorted)[names(df_sorted) == country] &lt;- \"Numbers\"\n    \n    # Append the sorted data to the result dataframe\n    test &lt;- rbind(test, df_sorted)\n  }\n  \n  # Convert the results to a dataframe and return it\n  top_causes_df &lt;- as.data.frame(test)\n}\n\n# Function to extract the second year from a filename string (if it contains multiple years)\nget_second_year &lt;- function(string) {\n  # Find all four-digit numbers (representing years) in the filename\n  matches &lt;- regmatches(string, gregexpr(\"\\\\d{4}\", string))\n  \n  # Return the second year if it exists, otherwise return NA\n  if (length(matches[[1]]) &gt;= 2) {\n    return(matches[[1]][2])\n  } else {\n    return(NA)\n  }\n}\n\n# Set the working directory to the folder containing the Excel files\npath &lt;- \"D:/Data_Projects/Tableau/WHO_Death/Data/Raw_Year_xl\"\nsetwd(path)\n\n# Get a list of all files in the directory\nrlist &lt;- list.files(\".\")\n\n# Apply the get_second_year function to each file to extract the year information\nYears &lt;- sapply(rlist, get_second_year)\n\n# Initialize an empty dataframe to store all the data from multiple years\nall_data &lt;- data.frame()\n\n# Initialize a counter to track the current file\ncount &lt;- 1\n\n# Loop through each year and process the corresponding Excel file\nfor (year in Years) {\n  # Read, clean, and build the dataset for the current year\n  df_year &lt;- build_year_df(clean_sheet(read_sheet(rlist[count])), year)\n  \n  # Append the cleaned and processed data to the main dataframe\n  all_data &lt;- rbind(all_data, df_year)\n  \n  # Increment the counter to move to the next file\n  count &lt;- count + 1\n}\n\n# Write the combined data for all years and countries to a CSV file\nwrite.csv(all_data, file = \"Top_5_All_Years.csv\")\nThe final CSV looked like this:\n\n\n\nimage\n\n\n\n\n\nAs we have data separated by space, time, and category, two types of charts immediately come to mind. Firstly, a world map to cover the spatial dimension and secondly a stacked bar chart of causes of death per year which takes care of time and category. I wanted the user to be able to select multiple countries and be able to visually understand trends in major causes of death as well as being able to select causes of death to be highlighted across the data. This posed the perfect challenge as it requires some tinkering across multiple chart and data types as well as actions, expressions, filters, parameters, and calculated fields. Thus, a relatively simple dashboard can have some interesting behaviors that users might not often get to see.\n\n\n\n\n\n\nimage\n\n\nTableau is awesome for auto-generating simple map based charts. Simply butting the country column from the CSV data source into the details section created a monocolor map. I wanted to add color in a meaningful way without overwhelming the dashboard user. To do so I decided that instead of using some kind of multi color scheme to represent multiple causes of death per country over the six years, I would highlight each country via its top cause of death for a year selected by the user. Making a Year Selected parameter that the user could change was easy but finding the top cause of death for each year for each country was a little tricky. Tableau uses its bespoke Tableau Calculation Language (TCL) for internal data wrangling and it definitely has some quirks. As such I usually put in a couple layers of redundancy to catch errors before they happen. First, I created a calculated field that grabbed the cause of death numbers for a chosen value of the Selected Year parameter:\n\n\n\nimage\n\n\nNext I used the following expression:\n\n\n\nimage\n\n\nThe first line iterates, grabbing unique combinations of Country, COD, and Selected Year and returns their associated number of death values. The second line is basically the same thing but finds the maximum total number of deaths for each country across all causes of death. It does this by first calculating the sum of deaths for each combination of Country and Cause of Death (like the first expression) and then taking the maximum of those sums for each country. By comparing these two expressions we return true when a unique combination of Country, COD, and Selected Year has a sum equal to the maximum sum for that country. This gives us the top cause of death per country for a given year.\nFrom there, I used this boolean to drive map coloring and tooltips, so clicking a country both filters the bar chart and highlights that country’s top COD.\n\n\n\nThe second major view is a stacked bar chart of causes of death by year. It uses the same Top_5_All_Years.csv data source, filtered down to the top causes per country:\n\nBars encode the total number of deaths.\nColors encode the cause of death.\nFilters let the user focus on selected countries and causes.\n\nWith parameter actions and highlight actions, clicking a bar segment highlights that cause across all selected countries and years, giving a quick sense of how a particular COD behaves globally.\n\n\n\nA small but important UX touch is the dynamic guidance in chart titles. When nothing is selected, titles instruct the user to click a country or bar. Once a country is selected, the titles update to show the selection. Clearing the selection restores the helper text instead of leaving blank or confusing views.\nTo do this, I used a Country Selected parameter and a worksheet action that resets the parameter when the selection is cleared. The dashboard action looks like:\n\n\n\nimage\n\n\nIt’s a small detail, but it makes the dashboard much more approachable. Tableau makes “good enough” visuals easy; polishing the UX requires a bit more effort and an understanding of how its calculation language and actions work together.\nFor me, this project was as much about sharpening my Tableau skills (parameters, actions, and TCL quirks) as it was about the mortality data itself.\nLink to the Tableau Dashboard"
  }
]